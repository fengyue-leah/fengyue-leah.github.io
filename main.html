<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Yue Feng</title>
  <meta name="description" content="Yue Feng's personal website
">

  <!--<link rel="shortcut icon" href="/assets/img/favicon.ico">-->

  <link rel="stylesheet" href="assets/css/main.css">
  <link rel="canonical" href="/">
</head>



  <body>

    <header class="site-header">

  <div class="wrapper">



    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">Bio</a>

        <!-- Blog -->


        <!-- Pages -->








            <a class="page-link" href="/publications/">Publications</a>





        <!-- CV link -->


      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Yue Feng (冯悦)</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Yue Feng (冯悦) clearfix">

  <div class="profile col one right">

      <a href="/assets/img/Boston.jpg">
        <img class="one" src="/assets/img/Boston.jpg">
      </a>



    <div class="address">
        <center>



            <strong>Email:</strong> yue.feng.20@ucl.ac.uk
            <p></p>




            <a href="https://scholar.google.com/citations?hl=en&user=ZNOC0lYAAAAJ"  target="_blank" title="Google Scholar">Google Scholar</a>


             |

            <!--<a href="https://www.github.com/Michaelvll" target="_blank" title="GitHub">GitHub</a>-->


        </center>
    </div>

  </div>


<div class="content">
<p> Hi! I'm a first-year Ph.D. student in the <a href="https://www.ucl.ac.uk/computer-science/">
            Department of Computer Science</a> at <a href="https://www.ucl.ac.uk/">
        University College London</a>, affiliated with the <a href="http://wi.cs.ucl.ac.uk/">
        Web Intelligence Group</a> of the <a href="https://www.ucl.ac.uk/ai-centre/">Centre for Artificial Intelligence</a>.
        I am grateful to be advised by <a href="https://sites.google.com/site/emineyilmaz/">Prof. Emine Yilmaz</a> and to be supported by a UKRI funded <a href="https://www.ucl.ac.uk/ai-centre/study/foundational-artificial-intelligence-mphilphd">Centre for Doctoral Training in Foundational AI</a>.

<!--<p>Before coming to UCL, I was a master student majoring in computer science-->
    <!--at <a href="http://english.ict.cas.cn/">Institute of Computing Technology</a>,-->
    <!--<a href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences</a>,-->
    <!--advised by Prof. <a href="https://scholar.google.com/citations?user=su14mcEAAAAJ">Jun Xu</a> and-->
    <!--Prof. <a href="http://www.bigdatalab.ac.cn/~gjf/">Jiafeng Guo</a>.</p>-->


<p>I aim to design systems that robustly and efficiently learn to understand human languages
    and web data to the end of advancing artificial intelligence web service
    and web information processing. My current research interests lie in
    <strong>natural language processing</strong>, <strong>information retrieval</strong>,
    and <strong>machine learning</strong>.</p>

<p>Feel free to reach me at <strong>yue[dot]feng[dot]20[at]ucl[dot]ac[dot]uk</strong>, or take my CV.</p>

<h3 id="education">Education</h3>

<div id="education">
<div class="namecard">
<table>


    <tr>
    <td class="logo-img-td">
        <a href="https://berkeley.edu"><img src="assets/img/berkeley.png" width="100%" /></a>
    </td>
    <td class="logo-txt-td">
         <a href="https://www.ucl.ac.uk/">University College London</a>, UK<br />
         Ph.D. student at <a href="http://wi.cs.ucl.ac.uk/">WI Group</a>. Sep. 2020 - Present
    </td>
    </tr>

    <tr>
    <td class="logo-img-td">
        <a href="https://mit.edu"><img src="assets/img/mit.png" width="100%" /></a>
    </td>
    <td class="logo-txt-td">
         <a href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences</a>, China<br />
         Master in Computer Science, working with Prof. <a href="https://scholar.google.com/citations?user=su14mcEAAAAJ">Jun Xu</a> and Prof. <a href="http://www.bigdatalab.ac.cn/~gjf/">Jiafeng Guo</a>. Sep. 2016 - Jun. 2019.
    </td>
    </tr>

    <tr>
    <td class="logo-img-td">
        <a href="http://en.sjtu.edu.cn"><img src="assets/img/sjtu.png" width="100%" /></a>
    </td>
    <td class="logo-txt-td">
         <a href="http://en.hit.edu.cn/">Harbin Institute of Technology</a>, China<br />
         Bachelor in Computer Science, Sep. 2012 - Jun. 2016.
    </td>
    </tr>

</table>
</div>
</div>

<h3 id="publications">Publications</h3>
<ol class="bibliography"><li>

<div id="Wu:2020DataMix">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">

        <a href="/assets/papers/img/privacy_eccv.png">
            <img src="/assets/papers/img/privacy_eccv.png" width="100%" />
        </a>

    </td>
    <td class="txt-td">

            <span class="title"><strong>DataMix: Efficient Privacy-Preserving Edge-Cloud Inference</strong></span>
            <span class="author nolinkcolor">





                        <a href="http://zhijianliu.com/" target="_blank">Zhijian Liu*</a>,








                    <em><a href="">Zhanghao Wu*</a></em>,








                        <a href="http://people.csail.mit.edu/ganchuang/" target="_blank">Chuang Gan</a>,









                        <a href="https://lzhu.me/" target="_blank">Ligeng Zhu</a>,









                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a>





            </span>

            <span class="periodical">

            <em>In ECCV</em>


            2020


            </span>


        <span class="links">


            <a class="abstract">Abstract</a>




             |

            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123560562.pdf" target="_blank">Paper</a>





             |

            <a href="/assets/pdf/Presentation_Privacy-Preserving_Edge-Cloud_Inference.pdf" target="_blank">Slides</a>


             |

            <a href="https://www.youtube.com/watch?v=nLVWIl-h5Zc&amp;feature=youtu.be" target="_blank">Demo</a>



        </span>

        <!-- Hidden abstract block -->

        <span class="abstract hidden">
            <p>Deep neural networks are widely deployed on edge devices. Users either perform the inference locally (i.e., edge-based) or send the data to the cloud and run inference remotely (i.e., cloud-based). However, edge devices are heavily constrained by insufficient hardware resources and cannot afford to run large models; cloud servers, if not trustworthy, will raise serious privacy issues. In this paper, we mediate between the resource-constrained edge devices and the privacy-invasive cloud servers by introducing a novel privacy-preserving edge-cloud inference framework, DataMix. We off-load the majority of the computations to the cloud and leverage a pair of mixing and de-mixing operation, inspired by mixup, to protect the privacy of the data transmitted to the cloud. Our framework has three advantages. First, it is privacy-preserving as the mixing cannot be inverted without the user’s private mixing coefficients. Second, our framework is accuracy-preserving because our framework takes advantage of the space spanned by image mixing, and we train the model in a mixing-aware manner to maintain accuracy. Third, our solution is efficient on the edge since the majority of the workload is delegated to the cloud, and mixing and de-mixing introduce few extra computations. DataMix introduces small communication overhead and maintains high hardware utilization on the cloud. Extensive experiments on multiple computer vision and speech recognition datasets demonstrate that our framework can greatly reduce the local computations on the edge (to fewer than 20% of FLOPs) with negligible loss of accuracy and no leakages of private information.</p>
        </span>

    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2020hat">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">

        <a href="/assets/papers/img/hat.png">
            <img src="/assets/papers/img/hat.png" width="100%" />
        </a>

    </td>
    <td class="txt-td">

            <span class="title"><strong>HAT: Hardware-Aware Transformers for Efficient Natural Language Processing</strong></span>
            <span class="author nolinkcolor">





                        <a href="https://hanruiwang.me/" target="_blank">Hanrui Wang</a>,








                    <em><a href="">Zhanghao Wu</a></em>,








                        <a href="http://zhijianliu.com/" target="_blank">Zhijian Liu</a>,









                        <a href="https://scholar.google.com/citations?user=x-AvvrYAAAAJ&amp;hl=en&amp;authuser=1" target="_blank">Han Cai</a>,









                        <a href="https://lzhu.me/" target="_blank">Ligeng Zhu</a>,









                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a>





            </span>

            <span class="periodical">

            <em>In ACL</em>


            2020


            </span>


        <span class="links">


            <a class="abstract">Abstract</a>




             |

            <a href="https://arxiv.org/pdf/2005.14187.pdf" target="_blank">Paper</a>








             |

            <a href="https://hanlab.mit.edu/projects/hat/" target="_blank">Website</a>

        </span>

        <!-- Hidden abstract block -->

        <span class="abstract hidden">
            <p>Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT’14 translation task on Raspberry Pi-4, HAT can achieve 3× speedup, 3.7× smaller size over baseline Transformer; 2.7× speedup, 3.6× smaller size over Evolved Transformer with 12,041× less search cost and no performance loss. HAT code is open-sourced at <a href="https://github.com/mit-hanlab/hardware-aware-transformers.git">https://github.com/mit-hanlab/hardware-aware-transformers.git</a>.</p>
        </span>

    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2020efficient">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">

        <a href="/assets/papers/img/enmt.png">
            <img src="/assets/papers/img/enmt.png" width="100%" />
        </a>

    </td>
    <td class="txt-td">

            <span class="title"><strong>Lite Transformer with Long-Short Range Attention</strong></span>
            <span class="author nolinkcolor">




                    <em><a href="">Zhanghao Wu*</a></em>,








                        <a href="http://zhijianliu.com/" target="_blank">Zhijian Liu*</a>,









                        <a href="http://linji.me/" target="_blank">Ji Lin</a>,









                        <a href="https://deepai.org/profile/yujun-lin" target="_blank">Yujun Lin</a>,









                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a>





            </span>

            <span class="periodical">

            <em>In ICLR</em>


            2020


            </span>


        <span class="links">


            <a class="abstract">Abstract</a>




             |

            <a href="https://arxiv.org/pdf/2004.11886.pdf" target="_blank">Paper</a>





             |

            <a href="/assets/pdf/Presentation_LiteTransformer.pdf" target="_blank">Slides</a>




             |

            <a href="https://hanlab.mit.edu/projects/litetransformer/" target="_blank">Website</a>

        </span>

        <!-- Hidden abstract block -->

        <span class="abstract hidden">
            <p>Transformer has become ubiquitous in natural language processing (e.g., machine translation, question answering); however, it requires enormous amount of computations to achieve high performance, which makes it not suitable for mobile applications that are tightly constrained by the hardware resources and battery. In this paper, we present an efficient mobile NLP architecture, Lite Transformer to facilitate deploying mobile NLP applications on edge devices. The key primitive is the Long-Short Range Attention (LSRA), where one group of heads specializes in the local context modeling (by convolution) while another group specializes in the long-distance relationship modeling (by attention). Such specialization brings consistent improvement over the vanilla transformer on three well-established language tasks: machine translation, abstractive summarization, and language modeling. Under constrained resources (500M/100M MACs), Lite Transformer outperforms transformer on WMT’14 English-French by 1.2/1.7 BLEU, respectively. Lite Transformer reduces the computation of transformer base model by 2.5x with 0.3 BLEU score degradation. Combining with pruning and quantization, we further compressed the model size of Lite Transformer by 18.2x. For language modeling, Lite Transformer achieves 1.8 lower perplexity than the transformer at around 500M MACs. Notably, Lite Transformer outperforms the AutoML-based Evolved Transformer by 0.5 higher BLEU for the mobile NLP setting without the costly architecture search that requires more than 250 GPU years. Code has been made available at <a href="https://github.com/mit-han-lab/lite-transformer">https://github.com/mit-han-lab/lite-transformer</a>.</p>
        </span>

    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wang:2020da">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">

        <a href="/assets/papers/img/da_taslp.png">
            <img src="/assets/papers/img/da_taslp.png" width="100%" />
        </a>

    </td>
    <td class="txt-td">

            <span class="title"><strong>Data Augmentation Using Deep Generative Models for Embedding Based Speaker Recognition</strong></span>
            <span class="author nolinkcolor">





                        <a href="https://speechlab.sjtu.edu.cn/members/shuai-wang" target="_blank">Shuai Wang</a>,









                        Yexin Yang,








                    <em><a href="">Zhanghao Wu</a></em>,








                        <a href="https://speechlab.sjtu.edu.cn/members/yanmin_qian" target="_blank">Yanmin Qian</a>,









                        and <a href="https://speechlab.sjtu.edu.cn/members/kai_yu" target="_blank">Kai Yu</a>





            </span>

            <span class="periodical">

            <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>


            2020


            </span>


        <span class="links">


            <a class="abstract">Abstract</a>




             |

            <a href="https://ieeexplore.ieee.org/abstract/document/9167416" target="_blank">Paper</a>








        </span>

        <!-- Hidden abstract block -->

        <span class="abstract hidden">
            <p>Data augmentation is an effective method to improve the robustness of embedding based speaker verification systems, which could be applied to either the front-end speaker embedding extractor or the back-end PLDA. Different from the conventional augmentation methods such as manually adding noise or reverberation to the original audios, in this article, we propose to use deep generative models to directly generate more diverse speaker embeddings, which would be used for robust PLDA training. Conditional GAN, and VAE are designed, and investigated for different embedding types, including factor analysis based i-vector, TDNN based x-vector, and ResNet based r-vector. The proposed back-end augmentation methods are evaluated on NIST SRE 2016, and 2018 dataset. Within the popular x-vector, and r-vector framework, the experimental results show that our proposed methods can outperform the traditional audio based back-end augmentation method while different front-end augmentation methods are considered.</p>
        </span>

    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Wu:2019vaeda">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">

        <a href="/assets/papers/img/vae_da.png">
            <img src="/assets/papers/img/vae_da.png" width="100%" />
        </a>

    </td>
    <td class="txt-td">

            <span class="title"><strong>Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification</strong></span>
            <span class="author nolinkcolor">




                    <em><a href="">Zhanghao Wu</a></em>,








                        <a href="https://speechlab.sjtu.edu.cn/members/shuai-wang" target="_blank">Shuai Wang</a>,









                        <a href="https://speechlab.sjtu.edu.cn/members/yanmin_qian" target="_blank">Yanmin Qian</a>,









                        and <a href="https://speechlab.sjtu.edu.cn/members/kai_yu" target="_blank">Kai Yu</a>





            </span>

            <span class="periodical">

            <em>In Interspeech</em>


            2019


                <strong> (Oral)</strong>

            </span>


        <span class="links">


            <a class="abstract">Abstract</a>




             |

            <a href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/2248.pdf" target="_blank">Paper</a>





             |

            <a href="/assets/pdf/Presentations_VAE_DA.pdf" target="_blank">Slides</a>




        </span>

        <!-- Hidden abstract block -->

        <span class="abstract hidden">
            <p>Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and xvector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54% and 7.84%, and a more significant improvement for x-vector on those two languages with EERs of 11.86% and 4.20%.</p>
        </span>

    </td>
    </tr>
    </table>
</div>
</div>

</li>
<li>

<div id="Han:2019rc">
<div class="namecard">
    <table>
    <tr>
    <td class="img-td">

        <a href="/assets/papers/img/on_device.png">
            <img src="/assets/papers/img/on_device.png" width="100%" />
        </a>

    </td>
    <td class="txt-td">

            <span class="title"><strong>On-Device Image Classification with Proxyless Neural Architecture Search and Quantization-Aware Fine-Tuning</strong></span>
            <span class="author nolinkcolor">





                        <a href="https://scholar.google.com/citations?user=x-AvvrYAAAAJ&amp;hl=en&amp;authuser=1" target="_blank">Han Cai</a>,









                        <a href="https://sites.google.com/view/tianzhe-wang/home" target="_blank">Tianzhe Wang</a>,








                    <em><a href="">Zhanghao Wu</a></em>,








                        <a href="https://scholar.google.com/citations?user=c1-_-dUAAAAJ&amp;hl=en" target="_blank">Kuan Wang</a>,









                        <a href="http://linji.me/" target="_blank">Ji Lin</a>,









                        and <a href="https://songhan.mit.edu" target="_blank">Song Han</a>





            </span>

            <span class="periodical">

            <em>In ICCV workshop</em>


            2019


            </span>


        <span class="links">


            <a class="abstract">Abstract</a>




             |

            <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/LPCV/Cai_On-Device_Image_Classification_with_Proxyless_Neural_Architecture_Search_and_Quantization-Aware_ICCVW_2019_paper.pdf" target="_blank">Paper</a>





             |

            <a href="/assets/pdf/Presentation_on_device.pdf" target="_blank">Slides</a>




        </span>

        <!-- Hidden abstract block -->

        <span class="abstract hidden">
            <p>It is challenging to efficiently deploy deep learning models on resource-constrained hardware devices (e.g., mobile and IoT devices) with strict efficiency constraints (e.g., latency, energy consumption). We employ Proxyless Neural Architecture Search (ProxylessNAS) to auto design compact and specialized neural network architectures for the target hardware platform. ProxylessNAS makes latency differentiable, so we can optimize not only accuracy but also latency by gradient descent. Such direct optimization saves the search cost by 200x compared to conventional neural architecture search methods. Our work is followed by quantization-aware fine-tuning to further boost efficiency. In the Low Power Image Recognition Competition at CVPR’19, our solution won the 3rd place on the task of Real-Time Image Classification (online track).</p>
        </span>

    </td>
    </tr>
    </table>
</div>
</div>

</li></ol>

<h3 id="honors--award">Honors &amp; Award</h3>
<ul>
  <li><strong>1st place</strong>, in Visual Wake Words (VWW) Challenge of CVPR’19, 2019.</li>
  <li><strong>3rd place</strong>, in Low Power Image Recognition Challenge of CVPR’19 (1st place of academic groups), 2019.</li>
  <li><strong>Outstanding Winner</strong>,in Mathematical Contest in Modeling (top 0.5%), 2017.</li>
  <li><strong>Chinese National Scholarship</strong>, highest honor for undergraduates, top 0.2% nation wide, 2018 &amp; 2019.</li>
  <li><strong>Excellent Graduate Award of SJTU</strong>, the highest honor for graduates at SJTU, 2020.</li>
  <li><strong>Zhiyuan Outstanding Student Scholarship of SJTU</strong>, 16 graduates of SJTU Zhiyuan College, 2020.</li>
  <li><strong>Fan Hsu-Chi Chancellor’s Scholarship</strong>, top 0.1%of 17,000 students in SJTU, 2017.</li>
  <li><strong>Zhiyuan Honorary Scholarship</strong>, top 5% of 17,000 students in SJTU, 2016-2018.</li>
</ul>


</div>

  </article>





</div>

      </div>
    </div>


        <div class="wrapper">
<center>
    &copy; Copyright 2020 Zhanghao Wu.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a>.


</center>
</div>




    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-125868731-1', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
